{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7aa92385",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/future/u/alexder/anaconda3/envs/cs356v2/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "import torchvision\n",
    "import crypten\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms\n",
    "import tqdm\n",
    "import crypten.mpc as mpc\n",
    "import crypten.communicator as comm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9755dec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "crypten.init()\n",
    "\n",
    "ALICE = 0\n",
    "BOB = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cec0cf84",
   "metadata": {},
   "outputs": [],
   "source": [
    "subsample = 0.02\n",
    "            \n",
    "@mpc.run_multiprocess(world_size=2)\n",
    "def train():\n",
    "    \"\"\"Apply data labeling access control model\"\"\"\n",
    "    # Alice loads features, Bob loads labels\n",
    "    data_root_dir = './poisoned_data_samples/one_hot_target/data_samples_sub50x'\n",
    "    data_alice_enc = crypten.load_from_party(os.path.join(data_root_dir, '2/train/data_0.pth'), src=ALICE)\n",
    "    targets_alice_enc = crypten.load_from_party(os.path.join(data_root_dir, '2/train/targets_0.pth'), src=ALICE)\n",
    "    \n",
    "    data_bob_enc = crypten.load_from_party(os.path.join(data_root_dir, '2/train/data_1.pth'), src=BOB)\n",
    "    targets_bob_enc = crypten.load_from_party(os.path.join(data_root_dir, '2/train/targets_1.pth'), src=BOB)\n",
    "    \n",
    "    val_data_alice_enc = crypten.load_from_party(os.path.join(data_root_dir, '2/train/data_0.pth'), src=ALICE)\n",
    "    val_targets_alice_enc = crypten.load_from_party(os.path.join(data_root_dir, '2/train/targets_0.pth'), src=ALICE)\n",
    "    \n",
    "    val_data_bob_enc = crypten.load_from_party(os.path.join(data_root_dir, '2/train/data_1.pth'), src=BOB)\n",
    "    val_targets_bob_enc = crypten.load_from_party(os.path.join(data_root_dir, '2/train/targets_1.pth'), src=BOB)\n",
    "    \n",
    "    model = torchvision.models.resnet50(num_classes = 10)\n",
    "    model.conv1 = torch.nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "\n",
    "    all_data = crypten.cat([data_alice_enc, data_bob_enc], dim=0)\n",
    "    all_targets = crypten.cat([targets_alice_enc, targets_bob_enc], dim=0)\n",
    "    \n",
    "    all_val_data = crypten.cat([val_data_alice_enc, val_data_bob_enc], dim=0)\n",
    "    all_val_targets = crypten.cat([val_targets_alice_enc, val_targets_bob_enc], dim=0)\n",
    "    \n",
    "    \n",
    "    dummy_input = torch.empty(8, 1, 28, 28)\n",
    "    resnet_plaintext = torchvision.models.resnet18(num_classes = 10)\n",
    "    resnet_plaintext.conv1 = torch.nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "    model = crypten.nn.from_pytorch(resnet_plaintext, dummy_input)\n",
    "    model.encrypt()\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    loss = crypten.nn.MSELoss()\n",
    "    \n",
    "    lr = 0.001\n",
    "    num_epochs = 2\n",
    "    batch_size = 8\n",
    "    \n",
    "    num_batches = all_data.size(0) // batch_size\n",
    "    \n",
    "    batch_losses = []\n",
    "    \n",
    "    for i in range(num_epochs):\n",
    "        crypten.print(f\"Epoch {i}\")\n",
    "        for batch in range(num_batches):\n",
    "            \n",
    "            start, end = batch * batch_size, (batch + 1) * batch_size\n",
    "                                    \n",
    "            # construct CrypTensors out of training examples / labels\n",
    "            x_train = all_data[start:end]\n",
    "            y_train = all_targets[start:end]\n",
    "            #y_train = crypten.cryptensor(y_batch, requires_grad=True)\n",
    "            \n",
    "            # perform forward pass:\n",
    "            output = model(x_train)\n",
    "            loss_value = loss(output, y_train)\n",
    "            \n",
    "            # set gradients to \"zero\" \n",
    "            model.zero_grad()\n",
    "\n",
    "            # perform backward pass: \n",
    "            loss_value.backward()\n",
    "\n",
    "            # update parameters\n",
    "            model.update_parameters(lr)\n",
    "            \n",
    "            # Print progress every batch:\n",
    "            batch_loss = loss_value.get_plain_text().detach()\n",
    "            batch_losses.append(batch_loss)\n",
    "            crypten.print(f\"\\tBatch {(batch + 1)} of {num_batches} Loss {batch_loss.item():.4f}\")\n",
    "            \n",
    "    np.save('batch_losses_poisoned.npy', batch_losses)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "13a0d40d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/future/u/alexder/anaconda3/envs/cs356v2/lib/python3.9/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /opt/conda/conda-bld/pytorch_1623448238472/work/c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n",
      "/future/u/alexder/anaconda3/envs/cs356v2/lib/python3.9/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /opt/conda/conda-bld/pytorch_1623448238472/work/c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n",
      "/future/u/alexder/anaconda3/envs/cs356v2/lib/python3.9/site-packages/crypten/nn/onnx_converter.py:161: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1623448238472/work/torch/csrc/utils/tensor_numpy.cpp:180.)\n",
      "  param = torch.from_numpy(numpy_helper.to_array(node))\n",
      "/future/u/alexder/anaconda3/envs/cs356v2/lib/python3.9/site-packages/crypten/nn/onnx_converter.py:161: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1623448238472/work/torch/csrc/utils/tensor_numpy.cpp:180.)\n",
      "  param = torch.from_numpy(numpy_helper.to_array(node))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/future/u/alexder/anaconda3/envs/cs356v2/lib/python3.9/site-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\n",
      "To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /opt/conda/conda-bld/pytorch_1623448238472/work/aten/src/ATen/native/BinaryOps.cpp:467.)\n",
      "  return torch.floor_divide(self, other)\n",
      "/future/u/alexder/anaconda3/envs/cs356v2/lib/python3.9/site-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\n",
      "To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /opt/conda/conda-bld/pytorch_1623448238472/work/aten/src/ATen/native/BinaryOps.cpp:467.)\n",
      "  return torch.floor_divide(self, other)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tBatch 1 of 150 Loss 0.4636\n",
      "\tBatch 2 of 150 Loss 0.2796\n",
      "\tBatch 3 of 150 Loss 0.2518\n",
      "\tBatch 4 of 150 Loss 0.2974\n",
      "\tBatch 5 of 150 Loss 0.2151\n",
      "\tBatch 6 of 150 Loss 0.1841\n",
      "\tBatch 7 of 150 Loss 0.1503\n",
      "\tBatch 8 of 150 Loss 0.1087\n",
      "\tBatch 9 of 150 Loss 0.1299\n",
      "\tBatch 10 of 150 Loss 0.0957\n",
      "\tBatch 11 of 150 Loss 0.1197\n",
      "\tBatch 12 of 150 Loss 0.1255\n",
      "\tBatch 13 of 150 Loss 0.1273\n",
      "\tBatch 14 of 150 Loss 0.0877\n",
      "\tBatch 15 of 150 Loss 0.1204\n",
      "\tBatch 16 of 150 Loss 0.1036\n",
      "\tBatch 17 of 150 Loss 0.1134\n",
      "\tBatch 18 of 150 Loss 0.1094\n",
      "\tBatch 19 of 150 Loss 0.1013\n",
      "\tBatch 20 of 150 Loss 0.1041\n",
      "\tBatch 21 of 150 Loss 0.1100\n",
      "\tBatch 22 of 150 Loss 0.1022\n",
      "\tBatch 23 of 150 Loss 0.1122\n",
      "\tBatch 24 of 150 Loss 0.0929\n",
      "\tBatch 25 of 150 Loss 0.1027\n",
      "\tBatch 26 of 150 Loss 0.1104\n",
      "\tBatch 27 of 150 Loss 0.1040\n",
      "\tBatch 28 of 150 Loss 0.0814\n",
      "\tBatch 29 of 150 Loss 0.1071\n",
      "\tBatch 30 of 150 Loss 0.0907\n",
      "\tBatch 31 of 150 Loss 0.1019\n",
      "\tBatch 32 of 150 Loss 0.1039\n",
      "\tBatch 33 of 150 Loss 0.1141\n",
      "\tBatch 34 of 150 Loss 0.1024\n",
      "\tBatch 35 of 150 Loss 0.1020\n",
      "\tBatch 36 of 150 Loss 0.0934\n",
      "\tBatch 37 of 150 Loss 0.1062\n",
      "\tBatch 38 of 150 Loss 0.0985\n",
      "\tBatch 39 of 150 Loss 0.0907\n",
      "\tBatch 40 of 150 Loss 0.0898\n",
      "\tBatch 41 of 150 Loss 0.1047\n",
      "\tBatch 42 of 150 Loss 0.0955\n",
      "\tBatch 43 of 150 Loss 0.1025\n",
      "\tBatch 44 of 150 Loss 0.1187\n",
      "\tBatch 45 of 150 Loss 0.1039\n",
      "\tBatch 46 of 150 Loss 0.0929\n",
      "\tBatch 47 of 150 Loss 0.0829\n",
      "\tBatch 48 of 150 Loss 0.0882\n",
      "\tBatch 49 of 150 Loss 0.0742\n",
      "\tBatch 50 of 150 Loss 0.0909\n",
      "\tBatch 51 of 150 Loss 0.0986\n",
      "\tBatch 52 of 150 Loss 0.1129\n",
      "\tBatch 53 of 150 Loss 0.1053\n",
      "\tBatch 54 of 150 Loss 0.1017\n",
      "\tBatch 55 of 150 Loss 0.1042\n",
      "\tBatch 56 of 150 Loss 0.1052\n",
      "\tBatch 57 of 150 Loss 0.1023\n",
      "\tBatch 58 of 150 Loss 0.1015\n",
      "\tBatch 59 of 150 Loss 0.1123\n",
      "\tBatch 60 of 150 Loss 0.1013\n",
      "\tBatch 61 of 150 Loss 0.1217\n",
      "\tBatch 62 of 150 Loss 0.1045\n",
      "\tBatch 63 of 150 Loss 0.1194\n",
      "\tBatch 64 of 150 Loss 0.1138\n",
      "\tBatch 65 of 150 Loss 0.0902\n",
      "\tBatch 66 of 150 Loss 0.0931\n",
      "\tBatch 67 of 150 Loss 0.0932\n",
      "\tBatch 68 of 150 Loss 0.0946\n",
      "\tBatch 69 of 150 Loss 0.0960\n",
      "\tBatch 70 of 150 Loss 0.0856\n",
      "\tBatch 71 of 150 Loss 0.1122\n",
      "\tBatch 72 of 150 Loss 0.1011\n",
      "\tBatch 73 of 150 Loss 0.1032\n",
      "\tBatch 74 of 150 Loss 0.0880\n",
      "\tBatch 75 of 150 Loss 0.1136\n",
      "\tBatch 76 of 150 Loss 0.1051\n",
      "\tBatch 77 of 150 Loss 0.1205\n",
      "\tBatch 78 of 150 Loss 0.1351\n",
      "\tBatch 79 of 150 Loss 0.1199\n",
      "\tBatch 80 of 150 Loss 0.1077\n",
      "\tBatch 81 of 150 Loss 0.1197\n",
      "\tBatch 82 of 150 Loss 0.1323\n",
      "\tBatch 83 of 150 Loss 0.1085\n",
      "\tBatch 84 of 150 Loss 0.1196\n",
      "\tBatch 85 of 150 Loss 0.1573\n",
      "\tBatch 86 of 150 Loss 0.1114\n",
      "\tBatch 87 of 150 Loss 0.1096\n",
      "\tBatch 88 of 150 Loss 0.1031\n",
      "\tBatch 89 of 150 Loss 0.1060\n",
      "\tBatch 90 of 150 Loss 0.1031\n",
      "\tBatch 91 of 150 Loss 0.1230\n",
      "\tBatch 92 of 150 Loss 0.1293\n",
      "\tBatch 93 of 150 Loss 0.1208\n",
      "\tBatch 94 of 150 Loss 0.1271\n",
      "\tBatch 95 of 150 Loss 0.1296\n",
      "\tBatch 96 of 150 Loss 0.1140\n",
      "\tBatch 97 of 150 Loss 0.1182\n",
      "\tBatch 98 of 150 Loss 0.1421\n",
      "\tBatch 99 of 150 Loss 0.1011\n",
      "\tBatch 100 of 150 Loss 0.1256\n",
      "\tBatch 101 of 150 Loss 0.0958\n",
      "\tBatch 102 of 150 Loss 0.1028\n",
      "\tBatch 103 of 150 Loss 0.1152\n",
      "\tBatch 104 of 150 Loss 0.0928\n",
      "\tBatch 105 of 150 Loss 0.1090\n",
      "\tBatch 106 of 150 Loss 0.1036\n",
      "\tBatch 107 of 150 Loss 0.1016\n",
      "\tBatch 108 of 150 Loss 0.1226\n",
      "\tBatch 109 of 150 Loss 0.1131\n",
      "\tBatch 110 of 150 Loss 0.1240\n",
      "\tBatch 111 of 150 Loss 0.1164\n",
      "\tBatch 112 of 150 Loss 0.1120\n",
      "\tBatch 113 of 150 Loss 0.1309\n",
      "\tBatch 114 of 150 Loss 0.1052\n",
      "\tBatch 115 of 150 Loss 0.1155\n",
      "\tBatch 116 of 150 Loss 0.1245\n",
      "\tBatch 117 of 150 Loss 0.1147\n",
      "\tBatch 118 of 150 Loss 0.0875\n",
      "\tBatch 119 of 150 Loss 0.1193\n",
      "\tBatch 120 of 150 Loss 0.1121\n",
      "\tBatch 121 of 150 Loss 0.1033\n",
      "\tBatch 122 of 150 Loss 0.1112\n",
      "\tBatch 123 of 150 Loss 0.1205\n",
      "\tBatch 124 of 150 Loss 0.1182\n",
      "\tBatch 125 of 150 Loss 0.1064\n",
      "\tBatch 126 of 150 Loss 0.0997\n",
      "\tBatch 127 of 150 Loss 0.1054\n",
      "\tBatch 128 of 150 Loss 0.1160\n",
      "\tBatch 129 of 150 Loss 0.1171\n",
      "\tBatch 130 of 150 Loss 0.1036\n",
      "\tBatch 131 of 150 Loss 0.1344\n",
      "\tBatch 132 of 150 Loss 0.1022\n",
      "\tBatch 133 of 150 Loss 0.1302\n",
      "\tBatch 134 of 150 Loss 0.1067\n",
      "\tBatch 135 of 150 Loss 0.0917\n",
      "\tBatch 136 of 150 Loss 0.1044\n",
      "\tBatch 137 of 150 Loss 0.1257\n",
      "\tBatch 138 of 150 Loss 0.1031\n",
      "\tBatch 139 of 150 Loss 0.1200\n",
      "\tBatch 140 of 150 Loss 0.1098\n",
      "\tBatch 141 of 150 Loss 0.1008\n",
      "\tBatch 142 of 150 Loss 0.1108\n",
      "\tBatch 143 of 150 Loss 0.0974\n",
      "\tBatch 144 of 150 Loss 0.1025\n",
      "\tBatch 145 of 150 Loss 0.1073\n",
      "\tBatch 146 of 150 Loss 0.1279\n",
      "\tBatch 147 of 150 Loss 0.1180\n",
      "\tBatch 148 of 150 Loss 0.1135\n",
      "\tBatch 149 of 150 Loss 0.1259\n",
      "\tBatch 150 of 150 Loss 0.1195\n",
      "Epoch 1\n",
      "\tBatch 1 of 150 Loss 0.1085\n",
      "\tBatch 2 of 150 Loss 0.0993\n",
      "\tBatch 3 of 150 Loss 0.1076\n",
      "\tBatch 4 of 150 Loss 0.1172\n",
      "\tBatch 5 of 150 Loss 0.1028\n",
      "\tBatch 6 of 150 Loss 0.1044\n",
      "\tBatch 7 of 150 Loss 0.1063\n",
      "\tBatch 8 of 150 Loss 0.0929\n",
      "\tBatch 9 of 150 Loss 0.0930\n",
      "\tBatch 10 of 150 Loss 0.0835\n",
      "\tBatch 11 of 150 Loss 0.0958\n",
      "\tBatch 12 of 150 Loss 0.1006\n",
      "\tBatch 13 of 150 Loss 0.1165\n",
      "\tBatch 14 of 150 Loss 0.0890\n",
      "\tBatch 15 of 150 Loss 0.1095\n",
      "\tBatch 16 of 150 Loss 0.0993\n",
      "\tBatch 17 of 150 Loss 0.1063\n",
      "\tBatch 18 of 150 Loss 0.1050\n",
      "\tBatch 19 of 150 Loss 0.0966\n",
      "\tBatch 20 of 150 Loss 0.1051\n",
      "\tBatch 21 of 150 Loss 0.1047\n",
      "\tBatch 22 of 150 Loss 0.0986\n",
      "\tBatch 23 of 150 Loss 0.1078\n",
      "\tBatch 24 of 150 Loss 0.0892\n",
      "\tBatch 25 of 150 Loss 0.0989\n",
      "\tBatch 26 of 150 Loss 0.1063\n",
      "\tBatch 27 of 150 Loss 0.0979\n",
      "\tBatch 28 of 150 Loss 0.0787\n",
      "\tBatch 29 of 150 Loss 0.1040\n",
      "\tBatch 30 of 150 Loss 0.0890\n",
      "\tBatch 31 of 150 Loss 0.0984\n",
      "\tBatch 32 of 150 Loss 0.1004\n",
      "\tBatch 33 of 150 Loss 0.1092\n",
      "\tBatch 34 of 150 Loss 0.0995\n",
      "\tBatch 35 of 150 Loss 0.0987\n",
      "\tBatch 36 of 150 Loss 0.0906\n",
      "\tBatch 37 of 150 Loss 0.1006\n",
      "\tBatch 38 of 150 Loss 0.0949\n",
      "\tBatch 39 of 150 Loss 0.0885\n",
      "\tBatch 40 of 150 Loss 0.0869\n",
      "\tBatch 41 of 150 Loss 0.1021\n",
      "\tBatch 42 of 150 Loss 0.0935\n",
      "\tBatch 43 of 150 Loss 0.0991\n",
      "\tBatch 44 of 150 Loss 0.1132\n",
      "\tBatch 45 of 150 Loss 0.1001\n",
      "\tBatch 46 of 150 Loss 0.0899\n",
      "\tBatch 47 of 150 Loss 0.0813\n",
      "\tBatch 48 of 150 Loss 0.0860\n",
      "\tBatch 49 of 150 Loss 0.0729\n",
      "\tBatch 50 of 150 Loss 0.0885\n",
      "\tBatch 51 of 150 Loss 0.0970\n",
      "\tBatch 52 of 150 Loss 0.1087\n",
      "\tBatch 53 of 150 Loss 0.1021\n",
      "\tBatch 54 of 150 Loss 0.1008\n",
      "\tBatch 55 of 150 Loss 0.1012\n",
      "\tBatch 56 of 150 Loss 0.1018\n",
      "\tBatch 57 of 150 Loss 0.0993\n",
      "\tBatch 58 of 150 Loss 0.0976\n",
      "\tBatch 59 of 150 Loss 0.1064\n",
      "\tBatch 60 of 150 Loss 0.0977\n",
      "\tBatch 61 of 150 Loss 0.1181\n",
      "\tBatch 62 of 150 Loss 0.1006\n",
      "\tBatch 63 of 150 Loss 0.1151\n",
      "\tBatch 64 of 150 Loss 0.1090\n",
      "\tBatch 65 of 150 Loss 0.0865\n",
      "\tBatch 66 of 150 Loss 0.0878\n",
      "\tBatch 67 of 150 Loss 0.0905\n",
      "\tBatch 68 of 150 Loss 0.0924\n",
      "\tBatch 69 of 150 Loss 0.0923\n",
      "\tBatch 70 of 150 Loss 0.0825\n",
      "\tBatch 71 of 150 Loss 0.1088\n",
      "\tBatch 72 of 150 Loss 0.0982\n",
      "\tBatch 73 of 150 Loss 0.0991\n",
      "\tBatch 74 of 150 Loss 0.0849\n",
      "\tBatch 75 of 150 Loss 0.1069\n",
      "\tBatch 76 of 150 Loss 0.1015\n",
      "\tBatch 77 of 150 Loss 0.1152\n",
      "\tBatch 78 of 150 Loss 0.1299\n",
      "\tBatch 79 of 150 Loss 0.1162\n",
      "\tBatch 80 of 150 Loss 0.1031\n",
      "\tBatch 81 of 150 Loss 0.1151\n",
      "\tBatch 82 of 150 Loss 0.1280\n",
      "\tBatch 83 of 150 Loss 0.1039\n",
      "\tBatch 84 of 150 Loss 0.1163\n",
      "\tBatch 85 of 150 Loss 0.1514\n",
      "\tBatch 86 of 150 Loss 0.1080\n",
      "\tBatch 87 of 150 Loss 0.1068\n",
      "\tBatch 88 of 150 Loss 0.0993\n",
      "\tBatch 89 of 150 Loss 0.1034\n",
      "\tBatch 90 of 150 Loss 0.0988\n",
      "\tBatch 91 of 150 Loss 0.1168\n",
      "\tBatch 92 of 150 Loss 0.1241\n",
      "\tBatch 93 of 150 Loss 0.1179\n",
      "\tBatch 94 of 150 Loss 0.1223\n",
      "\tBatch 95 of 150 Loss 0.1246\n",
      "\tBatch 96 of 150 Loss 0.1082\n",
      "\tBatch 97 of 150 Loss 0.1127\n",
      "\tBatch 98 of 150 Loss 0.1353\n",
      "\tBatch 99 of 150 Loss 0.0981\n",
      "\tBatch 100 of 150 Loss 0.1214\n",
      "\tBatch 101 of 150 Loss 0.0919\n",
      "\tBatch 102 of 150 Loss 0.0978\n",
      "\tBatch 103 of 150 Loss 0.1102\n",
      "\tBatch 104 of 150 Loss 0.0901\n",
      "\tBatch 105 of 150 Loss 0.1051\n",
      "\tBatch 106 of 150 Loss 0.1003\n",
      "\tBatch 107 of 150 Loss 0.0975\n",
      "\tBatch 108 of 150 Loss 0.1175\n",
      "\tBatch 109 of 150 Loss 0.1103\n",
      "\tBatch 110 of 150 Loss 0.1178\n",
      "\tBatch 111 of 150 Loss 0.1117\n",
      "\tBatch 112 of 150 Loss 0.1053\n",
      "\tBatch 113 of 150 Loss 0.1267\n",
      "\tBatch 114 of 150 Loss 0.1013\n",
      "\tBatch 115 of 150 Loss 0.1106\n",
      "\tBatch 116 of 150 Loss 0.1184\n",
      "\tBatch 117 of 150 Loss 0.1100\n",
      "\tBatch 118 of 150 Loss 0.0849\n",
      "\tBatch 119 of 150 Loss 0.1142\n",
      "\tBatch 120 of 150 Loss 0.1087\n",
      "\tBatch 121 of 150 Loss 0.1014\n",
      "\tBatch 122 of 150 Loss 0.1067\n",
      "\tBatch 123 of 150 Loss 0.1165\n",
      "\tBatch 124 of 150 Loss 0.1138\n",
      "\tBatch 125 of 150 Loss 0.1035\n",
      "\tBatch 126 of 150 Loss 0.0973\n",
      "\tBatch 127 of 150 Loss 0.1015\n",
      "\tBatch 128 of 150 Loss 0.1129\n",
      "\tBatch 129 of 150 Loss 0.1131\n",
      "\tBatch 130 of 150 Loss 0.1018\n",
      "\tBatch 131 of 150 Loss 0.1289\n",
      "\tBatch 132 of 150 Loss 0.0999\n",
      "\tBatch 133 of 150 Loss 0.1232\n",
      "\tBatch 134 of 150 Loss 0.1032\n",
      "\tBatch 135 of 150 Loss 0.0883\n",
      "\tBatch 136 of 150 Loss 0.1014\n",
      "\tBatch 137 of 150 Loss 0.1209\n",
      "\tBatch 138 of 150 Loss 0.0996\n",
      "\tBatch 139 of 150 Loss 0.1147\n",
      "\tBatch 140 of 150 Loss 0.1059\n",
      "\tBatch 141 of 150 Loss 0.0976\n",
      "\tBatch 142 of 150 Loss 0.1056\n",
      "\tBatch 143 of 150 Loss 0.0943\n",
      "\tBatch 144 of 150 Loss 0.0984\n",
      "\tBatch 145 of 150 Loss 0.1047\n",
      "\tBatch 146 of 150 Loss 0.1224\n",
      "\tBatch 147 of 150 Loss 0.1139\n",
      "\tBatch 148 of 150 Loss 0.1089\n",
      "\tBatch 149 of 150 Loss 0.1212\n",
      "\tBatch 150 of 150 Loss 0.1153\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
